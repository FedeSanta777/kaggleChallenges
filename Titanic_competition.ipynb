{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 3136,
          "databundleVersionId": 26502,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30458,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Titanic competition w/ TensorFlow Decision Forests",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FedeSanta777/kaggleChallenges/blob/main/Titanic_competition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'titanic:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F3136%2F26502%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240328%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240328T122538Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D146d6bb1b7715df1813d77b6f5b6b2d8107f3e3dec2c8540eb4f31bcd186f7ad1fb8ae704e4f2e5fb45693ebdb5fa8cf0f531f71fd22993001e6b5d052bf32164b7c4732b2449bf8cd25f0ca5b55af45e4ae04a3f70fbb07c509d89e3da2797cf6f778c1ef7188fcff32ddbb88ab66aa750c51a894638d207975dd472eedc06f0afdb4bb9c634033e4f6168c70895636060819a1d3a9ce15d3fe9de9dcb5142ff2d5d70a4175993a1acc00d3f401af4e5cea60587bf6b058630d7d96c970b0c93462b0a5dc57ca5c93b0da48f61b0362acf9f83437e8085cec56c65a7dab69f65f857cf442740a261bc7e1ac06a61ad5d27d265d3c40fb8ecd3a8b53f61d2343'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "LXfm-G0jBdrL"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Titanic competition with TensorFlow Decision Forests\n",
        "\n",
        "This notebook will take you through the steps needed to train a baseline Gradient Boosted Trees Model using TensorFlow Decision Forests and creating a submission on the Titanic competition.\n",
        "\n",
        "This notebook shows:\n",
        "\n",
        "1. How to do some basic pre-processing. For example, the passenger names will be tokenized, and ticket names will be splitted in parts.\n",
        "1. How to train a Gradient Boosted Trees (GBT) with default parameters\n",
        "1. How to train a GBT with improved default parameters\n",
        "1. How to tune the parameters of a GBTs\n",
        "1. How to train and ensemble many GBTs"
      ],
      "metadata": {
        "id": "FG6AF8dNBdrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports dependencies"
      ],
      "metadata": {
        "id": "6I0TnTseBdrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_decision_forests as tfdf\n",
        "\n",
        "print(f\"Found TF-DF {tfdf.__version__}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T15:32:04.264037Z",
          "iopub.execute_input": "2023-04-17T15:32:04.26451Z",
          "iopub.status.idle": "2023-04-17T15:32:04.272355Z",
          "shell.execute_reply.started": "2023-04-17T15:32:04.264459Z",
          "shell.execute_reply": "2023-04-17T15:32:04.270819Z"
        },
        "trusted": true,
        "id": "nc2CozCvBdrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset"
      ],
      "metadata": {
        "id": "ZVLHqFGMBdrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
        "serving_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n",
        "\n",
        "train_df.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T15:30:04.102889Z",
          "iopub.execute_input": "2023-04-17T15:30:04.10358Z",
          "iopub.status.idle": "2023-04-17T15:30:04.158126Z",
          "shell.execute_reply.started": "2023-04-17T15:30:04.103539Z",
          "shell.execute_reply": "2023-04-17T15:30:04.156877Z"
        },
        "trusted": true,
        "id": "4_G2JTpzBdrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare dataset\n",
        "\n",
        "We will apply the following transformations on the dataset.\n",
        "\n",
        "1. Tokenize the names. For example, \"Braund, Mr. Owen Harris\" will become [\"Braund\", \"Mr.\", \"Owen\", \"Harris\"].\n",
        "2. Extract any prefix in the ticket. For example ticket \"STON/O2. 3101282\" will become \"STON/O2.\" and 3101282."
      ],
      "metadata": {
        "id": "7-9nnqAjBdrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    def normalize_name(x):\n",
        "        return \" \".join([v.strip(\",()[].\\\"'\") for v in x.split(\" \")])\n",
        "\n",
        "    def ticket_number(x):\n",
        "        return x.split(\" \")[-1]\n",
        "\n",
        "    def ticket_item(x):\n",
        "        items = x.split(\" \")\n",
        "        if len(items) == 1:\n",
        "            return \"NONE\"\n",
        "        return \"_\".join(items[0:-1])\n",
        "\n",
        "    df[\"Name\"] = df[\"Name\"].apply(normalize_name)\n",
        "    df[\"Ticket_number\"] = df[\"Ticket\"].apply(ticket_number)\n",
        "    df[\"Ticket_item\"] = df[\"Ticket\"].apply(ticket_item)\n",
        "    return df\n",
        "\n",
        "preprocessed_train_df = preprocess(train_df)\n",
        "preprocessed_serving_df = preprocess(serving_df)\n",
        "\n",
        "preprocessed_train_df.head(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T15:30:09.50201Z",
          "iopub.execute_input": "2023-04-17T15:30:09.502467Z",
          "iopub.status.idle": "2023-04-17T15:30:09.540151Z",
          "shell.execute_reply.started": "2023-04-17T15:30:09.502432Z",
          "shell.execute_reply": "2023-04-17T15:30:09.538948Z"
        },
        "trusted": true,
        "id": "vL5DkdA_BdrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's keep the list of the input features of the model. Notably, we don't want to train our model on the \"PassengerId\" and \"Ticket\" features."
      ],
      "metadata": {
        "id": "qEDktBrpBdrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_features = list(preprocessed_train_df.columns)\n",
        "input_features.remove(\"Ticket\")\n",
        "input_features.remove(\"PassengerId\")\n",
        "input_features.remove(\"Survived\")\n",
        "#input_features.remove(\"Ticket_number\")\n",
        "\n",
        "print(f\"Input features: {input_features}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T15:30:14.000466Z",
          "iopub.execute_input": "2023-04-17T15:30:14.000868Z",
          "iopub.status.idle": "2023-04-17T15:30:14.00835Z",
          "shell.execute_reply.started": "2023-04-17T15:30:14.000833Z",
          "shell.execute_reply": "2023-04-17T15:30:14.006982Z"
        },
        "trusted": true,
        "id": "ljrIBqVyBdrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Pandas dataset to TensorFlow Dataset"
      ],
      "metadata": {
        "id": "HwlzNpxiBdrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_names(features, labels=None):\n",
        "    \"\"\"Divite the names into tokens. TF-DF can consume text tokens natively.\"\"\"\n",
        "    features[\"Name\"] =  tf.strings.split(features[\"Name\"])\n",
        "    return features, labels\n",
        "\n",
        "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_train_df,label=\"Survived\").map(tokenize_names)\n",
        "serving_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_serving_df).map(tokenize_names)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T15:30:17.896317Z",
          "iopub.execute_input": "2023-04-17T15:30:17.896741Z",
          "iopub.status.idle": "2023-04-17T15:30:18.231195Z",
          "shell.execute_reply.started": "2023-04-17T15:30:17.896705Z",
          "shell.execute_reply": "2023-04-17T15:30:18.229792Z"
        },
        "trusted": true,
        "id": "i51u-TzDBdrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model with default parameters\n",
        "\n",
        "### Train model\n",
        "\n",
        "First, we are training a GradientBoostedTreesModel model with the default parameters."
      ],
      "metadata": {
        "id": "L8TfmBkVBdrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tfdf.keras.GradientBoostedTreesModel(\n",
        "    verbose=0, # Very few logs\n",
        "    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n",
        "    exclude_non_specified_features=True, # Only use the features in \"features\"\n",
        "    random_seed=1234,\n",
        ")\n",
        "model.fit(train_ds)\n",
        "\n",
        "self_evaluation = model.make_inspector().evaluation()\n",
        "print(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T15:32:16.580089Z",
          "iopub.execute_input": "2023-04-17T15:32:16.581306Z",
          "iopub.status.idle": "2023-04-17T15:32:17.55132Z",
          "shell.execute_reply.started": "2023-04-17T15:32:16.581257Z",
          "shell.execute_reply": "2023-04-17T15:32:17.55024Z"
        },
        "trusted": true,
        "id": "fFCTkanuBdrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model with improved default parameters\n",
        "\n",
        "Now you'll use some specific parameters when creating the GBT model"
      ],
      "metadata": {
        "id": "b0OUICBYBdrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tfdf.keras.GradientBoostedTreesModel(\n",
        "    verbose=0, # Very few logs\n",
        "    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n",
        "    exclude_non_specified_features=True, # Only use the features in \"features\"\n",
        "\n",
        "    #num_trees=2000,\n",
        "\n",
        "    # Only for GBT.\n",
        "    # A bit slower, but great to understand the model.\n",
        "    # compute_permutation_variable_importance=True,\n",
        "\n",
        "    # Change the default hyper-parameters\n",
        "    # hyperparameter_template=\"benchmark_rank1@v1\",\n",
        "\n",
        "    #num_trees=1000,\n",
        "    #tuner=tuner\n",
        "\n",
        "    min_examples=1,\n",
        "    categorical_algorithm=\"RANDOM\",\n",
        "    #max_depth=4,\n",
        "    shrinkage=0.05,\n",
        "    #num_candidate_attributes_ratio=0.2,\n",
        "    split_axis=\"SPARSE_OBLIQUE\",\n",
        "    sparse_oblique_normalization=\"MIN_MAX\",\n",
        "    sparse_oblique_num_projections_exponent=2.0,\n",
        "    num_trees=2000,\n",
        "    #validation_ratio=0.0,\n",
        "    random_seed=1234,\n",
        "\n",
        ")\n",
        "model.fit(train_ds)\n",
        "\n",
        "self_evaluation = model.make_inspector().evaluation()\n",
        "print(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T15:06:14.822939Z",
          "iopub.execute_input": "2023-04-17T15:06:14.82343Z",
          "iopub.status.idle": "2023-04-17T15:06:16.103565Z",
          "shell.execute_reply.started": "2023-04-17T15:06:14.82339Z",
          "shell.execute_reply": "2023-04-17T15:06:16.102272Z"
        },
        "trusted": true,
        "id": "dNioJgo9BdrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the model and you can also notice the information about variable importance that the model figured out"
      ],
      "metadata": {
        "id": "HcF607n0BdrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T15:06:16.104878Z",
          "iopub.execute_input": "2023-04-17T15:06:16.105239Z",
          "iopub.status.idle": "2023-04-17T15:06:16.123439Z",
          "shell.execute_reply.started": "2023-04-17T15:06:16.105206Z",
          "shell.execute_reply": "2023-04-17T15:06:16.121993Z"
        },
        "trusted": true,
        "id": "Dsg5nScZBdrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make predictions"
      ],
      "metadata": {
        "id": "4HQ6E0o3BdrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_to_kaggle_format(model, threshold=0.5):\n",
        "    proba_survive = model.predict(serving_ds, verbose=0)[:,0]\n",
        "    return pd.DataFrame({\n",
        "        \"PassengerId\": serving_df[\"PassengerId\"],\n",
        "        \"Survived\": (proba_survive >= threshold).astype(int)\n",
        "    })\n",
        "\n",
        "def make_submission(kaggle_predictions):\n",
        "    path=\"/kaggle/working/submission.csv\"\n",
        "    kaggle_predictions.to_csv(path, index=False)\n",
        "    print(f\"Submission exported to {path}\")\n",
        "\n",
        "kaggle_predictions = prediction_to_kaggle_format(model)\n",
        "make_submission(kaggle_predictions)\n",
        "!head /kaggle/working/submission.csv"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T15:06:16.126575Z",
          "iopub.execute_input": "2023-04-17T15:06:16.126941Z",
          "iopub.status.idle": "2023-04-17T15:06:17.406876Z",
          "shell.execute_reply.started": "2023-04-17T15:06:16.126904Z",
          "shell.execute_reply": "2023-04-17T15:06:17.405022Z"
        },
        "trusted": true,
        "id": "i81FqkH2BdrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a model with hyperparameter tunning\n",
        "\n",
        "Hyper-parameter tuning is enabled by specifying the tuner constructor argument of the model. The tuner object contains all the configuration of the tuner (search space, optimizer, trial and objective).\n"
      ],
      "metadata": {
        "id": "4JbOn-pBBdrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = tfdf.tuner.RandomSearch(num_trials=1000)\n",
        "tuner.choice(\"min_examples\", [2, 5, 7, 10])\n",
        "tuner.choice(\"categorical_algorithm\", [\"CART\", \"RANDOM\"])\n",
        "\n",
        "local_search_space = tuner.choice(\"growing_strategy\", [\"LOCAL\"])\n",
        "local_search_space.choice(\"max_depth\", [3, 4, 5, 6, 8])\n",
        "\n",
        "global_search_space = tuner.choice(\"growing_strategy\", [\"BEST_FIRST_GLOBAL\"], merge=True)\n",
        "global_search_space.choice(\"max_num_nodes\", [16, 32, 64, 128, 256])\n",
        "\n",
        "#tuner.choice(\"use_hessian_gain\", [True, False])\n",
        "tuner.choice(\"shrinkage\", [0.02, 0.05, 0.10, 0.15])\n",
        "tuner.choice(\"num_candidate_attributes_ratio\", [0.2, 0.5, 0.9, 1.0])\n",
        "\n",
        "\n",
        "tuner.choice(\"split_axis\", [\"AXIS_ALIGNED\"])\n",
        "oblique_space = tuner.choice(\"split_axis\", [\"SPARSE_OBLIQUE\"], merge=True)\n",
        "oblique_space.choice(\"sparse_oblique_normalization\",\n",
        "                     [\"NONE\", \"STANDARD_DEVIATION\", \"MIN_MAX\"])\n",
        "oblique_space.choice(\"sparse_oblique_weights\", [\"BINARY\", \"CONTINUOUS\"])\n",
        "oblique_space.choice(\"sparse_oblique_num_projections_exponent\", [1.0, 1.5])\n",
        "\n",
        "# Tune the model. Notice the `tuner=tuner`.\n",
        "tuned_model = tfdf.keras.GradientBoostedTreesModel(tuner=tuner)\n",
        "tuned_model.fit(train_ds, verbose=0)\n",
        "\n",
        "tuned_self_evaluation = tuned_model.make_inspector().evaluation()\n",
        "print(f\"Accuracy: {tuned_self_evaluation.accuracy} Loss:{tuned_self_evaluation.loss}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T15:23:13.249675Z",
          "iopub.execute_input": "2023-04-17T15:23:13.251376Z",
          "iopub.status.idle": "2023-04-17T15:25:19.611729Z",
          "shell.execute_reply.started": "2023-04-17T15:23:13.251306Z",
          "shell.execute_reply": "2023-04-17T15:25:19.610154Z"
        },
        "trusted": true,
        "id": "-Q5WNAHGBdrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the last line in the cell above, you can see the accuracy is higher than previously with default parameters and parameters set by hand.\n",
        "\n",
        "This is the main idea behing hyperparameter tuning.\n",
        "\n",
        "For more information you can follow this tutorial: [Automated hyper-parameter tuning](https://www.tensorflow.org/decision_forests/tutorials/automatic_tuning_colab)"
      ],
      "metadata": {
        "id": "_bGYeiHtBdrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making an ensemble\n",
        "\n",
        "Here you'll create 100 models with different seeds and combine their results\n",
        "\n",
        "This approach removes a little bit the random aspects related to creating ML models\n",
        "\n",
        "In the GBT creation is used the `honest` parameter. It will use different training examples to infer the structure and the leaf values. This regularization technique trades examples for bias estimates."
      ],
      "metadata": {
        "id": "MbgiU_vtBdrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = None\n",
        "num_predictions = 0\n",
        "\n",
        "for i in range(100):\n",
        "    print(f\"i:{i}\")\n",
        "    # Possible models: GradientBoostedTreesModel or RandomForestModel\n",
        "    model = tfdf.keras.GradientBoostedTreesModel(\n",
        "        verbose=0, # Very few logs\n",
        "        features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n",
        "        exclude_non_specified_features=True, # Only use the features in \"features\"\n",
        "\n",
        "        #min_examples=1,\n",
        "        #categorical_algorithm=\"RANDOM\",\n",
        "        ##max_depth=4,\n",
        "        #shrinkage=0.05,\n",
        "        ##num_candidate_attributes_ratio=0.2,\n",
        "        #split_axis=\"SPARSE_OBLIQUE\",\n",
        "        #sparse_oblique_normalization=\"MIN_MAX\",\n",
        "        #sparse_oblique_num_projections_exponent=2.0,\n",
        "        #num_trees=2000,\n",
        "        ##validation_ratio=0.0,\n",
        "        random_seed=i,\n",
        "        honest=True,\n",
        "    )\n",
        "    model.fit(train_ds)\n",
        "\n",
        "    sub_predictions = model.predict(serving_ds, verbose=0)[:,0]\n",
        "    if predictions is None:\n",
        "        predictions = sub_predictions\n",
        "    else:\n",
        "        predictions += sub_predictions\n",
        "    num_predictions += 1\n",
        "\n",
        "predictions/=num_predictions\n",
        "\n",
        "kaggle_predictions = pd.DataFrame({\n",
        "        \"PassengerId\": serving_df[\"PassengerId\"],\n",
        "        \"Survived\": (predictions >= 0.5).astype(int)\n",
        "    })\n",
        "\n",
        "make_submission(kaggle_predictions)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-17T15:28:37.557745Z",
          "iopub.execute_input": "2023-04-17T15:28:37.558172Z",
          "iopub.status.idle": "2023-04-17T15:28:52.809698Z",
          "shell.execute_reply.started": "2023-04-17T15:28:37.55813Z",
          "shell.execute_reply": "2023-04-17T15:28:52.808193Z"
        },
        "trusted": true,
        "id": "63h7_1WZBdrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is next\n",
        "\n",
        "If you want to learn more about TensorFlow Decision Forests and its advanced features, you can follow the official documentation [here](https://www.tensorflow.org/decision_forests)"
      ],
      "metadata": {
        "id": "DkH7Yy1qBdrX"
      }
    }
  ]
}